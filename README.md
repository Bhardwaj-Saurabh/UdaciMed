# UdaciMed: Production-Ready Medical AI with Hardware-Aware Optimization

[![Python](https://img.shields.io/badge/Python-3.10-blue.svg)](https://www.python.org/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-orange.svg)](https://pytorch.org/)
[![ONNX](https://img.shields.io/badge/ONNX-Runtime-green.svg)](https://onnxruntime.ai/)
[![License](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE.md)

> **Optimizing ResNet-18 for pneumonia detection: Achieved 4/5 production targets through systematic hardware-aware optimization, delivering 2x throughput and 91.8% computational efficiency improvement while maintaining 99.23% clinical sensitivity.**

---

## ðŸŽ¯ Project Impact & Results

### Executive Summary
Developed and deployed an optimized deep learning pipeline for medical pneumonia detection that **achieved 80% of production targets** (4 out of 5 critical metrics) through systematic architectural optimization and hardware acceleration. The solution delivers **production-ready performance** for multi-tenant hospital deployment while maintaining strict clinical safety standards.

### Key Achievements

| Metric | Baseline | Optimized | Target | Status |
|--------|----------|-----------|--------|--------|
| **Memory Footprint** | 235 MB | **21.3 MB** | <100 MB | âœ… **89% under target** |
| **Throughput** | 127 sps | **4,010 sps** | >2,000 sps | âœ… **2x target exceeded** |
| **FLOP Efficiency** | 22.1 GFLOPs/sample | **0.15 GFLOPs/sample** | <0.4 | âœ… **91.8% reduction** |
| **Clinical Safety** | 98.5% | **99.2%** | >98% | âœ… **Maintained** |
| **Latency** | 7.87 ms | 3.74 ms | <3 ms | âš ï¸ 0.74 ms over |

**Production Readiness Score: 4/5 targets met (80%)** ðŸŽ‰

### Business Impact
- **31.5x throughput improvement** enables high-volume hospital screening workflows
- **11x memory reduction** allows multi-tenant GPU sharing and mobile deployment
- **99.23% sensitivity** exceeds clinical safety requirements for pneumonia detection
- **21.3 MB model size** enables edge deployment in resource-constrained rural clinics
- **Cross-platform ONNX format** supports diverse hospital infrastructure (GPU, CPU, mobile)

---

## ðŸ› ï¸ Technical Approach

### 1. Baseline Analysis & Bottleneck Identification
- **Profiled ResNet-18 architecture** using PyTorch Profiler and fvcore FLOPs counter
- **Identified critical bottleneck:** 64Ã—64 â†’ 224Ã—224 interpolation consumed 91.8% of computation
- **Analyzed hardware constraints** for GPU (T4), CPU (Intel), and mobile (ARM) deployment targets
- **Established performance baselines** across memory, throughput, latency, and clinical metrics

**Tools:** PyTorch Profiler, CUDA Memory Profiler, fvcore, medmnist dataset

### 2. Architecture Optimization (Phase 1)
Implemented **three distinct optimization techniques** with measurable impact:

#### âœ… Technique 1: Interpolation Removal
- **Modification:** Process images at native 64Ã—64 resolution instead of upscaling to 224Ã—224
- **Impact:** Eliminated 12.25x unnecessary pixel processing
- **Results:** 91.8% FLOP reduction, 2.6x speedup, **improved accuracy** (99.23% vs 98.46%)
- **Key Insight:** Medical imaging trained at native resolution doesn't need interpolation

#### âœ… Technique 2: Channels Last Memory Format
- **Modification:** Converted model to NHWC layout (`torch.channels_last`)
- **Impact:** Improved GPU cache utilization and memory access patterns
- **Results:** Better GPU efficiency, enables hardware acceleration
- **Key Insight:** Modern GPUs optimize for contiguous memory access in channel dimension

#### âœ… Technique 3: In-Place ReLU Operations
- **Modification:** Enabled `inplace=True` for all ReLU activations
- **Impact:** Reduced activation memory allocations
- **Results:** Lower memory footprint, faster execution
- **Key Insight:** Medical inference doesn't need intermediate activation storage

**Tools:** PyTorch, torchvision ResNet, custom optimization pipeline

### 3. Hardware Acceleration & Deployment (Phase 2)
- **ONNX Export:** Converted PyTorch model to ONNX with FP16 mixed precision
- **Dynamic Batching:** Configured flexible batch sizes (1, 8, 16, 32, 64) for diverse workflows
- **Execution Providers:** Tested CPU EP with fallback architecture for cross-platform compatibility
- **Benchmarking:** Systematic performance evaluation across batch sizes and hardware targets

**Key Results:**
- **Batch=1:** 268 sps, 3.74 ms latency (real-time emergency diagnosis)
- **Batch=64:** 4,010 sps, 15.96 ms latency (bulk screening, **2x target**)
- **Model Size:** 21.3 MB FP16 (vs 44 MB FP32)

**Tools:** ONNX Runtime, ONNX export, FP16 mixed precision, dynamic batching

### 4. Multi-Platform Deployment Strategy
Analyzed and documented deployment strategies for three target environments:

**ðŸ–¥ï¸ GPU Server Deployment (Triton + TensorRT)**
- Configured Triton Inference Server with TensorRT backend
- Implemented dynamic batching and FP16 optimization
- Designed multi-tenant architecture for hospital centralization

**ðŸ’» CPU Deployment (OpenVINO)**
- Evaluated ONNX Runtime vs Native OpenVINO IR
- Configured Intel-optimized execution (VNNI, AVX-512)
- Balanced clinical safety (FP32) vs performance (INT8)

**ðŸ“± Mobile/Edge Deployment (ONNX Mobile)**
- Compared cross-platform (ONNX Mobile) vs native (Core ML, LiteRT)
- Prioritized offline capability for rural clinic deployment
- Optimized for battery efficiency and model size constraints

---

## ðŸ§  Technical Skills Demonstrated

### Deep Learning & Model Optimization
- âœ… Hardware-aware architectural optimization
- âœ… Neural network profiling and bottleneck analysis
- âœ… Model quantization and mixed precision (FP16)
- âœ… Cross-platform deployment (ONNX, TensorRT, OpenVINO)
- âœ… Performance benchmarking and metrics evaluation

### Medical AI & Production Deployment
- âœ… Clinical safety validation (sensitivity/recall optimization)
- âœ… Multi-tenant deployment architecture
- âœ… Edge/mobile deployment strategy
- âœ… Production SLA achievement and optimization trade-offs
- âœ… Real-world constraint analysis (latency, throughput, memory)

### Tools & Frameworks
- **ML Frameworks:** PyTorch, torchvision, ONNX Runtime
- **Optimization:** fvcore (FLOPs), PyTorch Profiler, mixed precision (FP16)
- **Deployment:** ONNX, TensorRT, Triton Inference Server, OpenVINO
- **Medical Imaging:** medmnist, PneumoniaMNIST dataset, ROC/AUC analysis
- **Development:** Python, Jupyter, Git, performance profiling

---

## ðŸ“ Project Structure

```
udacimed-optimization/
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ 01_baseline_analysis.ipynb          # Profiling & bottleneck identification
â”‚   â”œâ”€â”€ 02_architecture_optimization.ipynb  # 3 optimization techniques + training
â”‚   â””â”€â”€ 03_deployment_acceleration.ipynb    # ONNX export + multi-platform analysis
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ architecture_optimization.py        # Optimization technique implementations
â”‚   â”œâ”€â”€ profiling.py                        # Performance measurement utilities
â”‚   â”œâ”€â”€ evaluation.py                       # Clinical metrics evaluation
â”‚   â””â”€â”€ model.py                            # ResNet-18 baseline + modifications
â”œâ”€â”€ results/
â”‚   â”œâ”€â”€ optimized_model.pth                 # Trained optimized weights
â”‚   â”œâ”€â”€ onnx_models/
â”‚   â”‚   â””â”€â”€ udacimed_pneumonia_optimized.onnx  # Production ONNX model (21.3 MB)
â”‚   â””â”€â”€ optimization_results_*.pkl          # Performance benchmarks
â””â”€â”€ README.md                                # This file
```

---

## ðŸš€ Quick Start

### Prerequisites
- Python 3.10+
- PyTorch 2.0+
- CUDA 11.8+ (optional, for GPU acceleration)
- 8GB RAM minimum

### Installation

```bash
# Clone repository
git clone <repository-url>
cd udacimed-optimization

# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -e .
```

### Run Notebooks

```bash
# Start Jupyter
jupyter notebook

# Open notebooks in sequence:
# 1. notebooks/01_baseline_analysis.ipynb
# 2. notebooks/02_architecture_optimization.ipynb
# 3. notebooks/03_deployment_acceleration.ipynb
```

### Run ONNX Inference (Production)

```python
import onnxruntime as ort
import numpy as np

# Load ONNX model
session = ort.InferenceSession(
    "results/onnx_models/udacimed_pneumonia_optimized.onnx",
    providers=['CUDAExecutionProvider', 'CPUExecutionProvider']
)

# Prepare input (batch_size, 3, 64, 64)
input_data = np.random.randn(64, 3, 64, 64).astype(np.float16)

# Run inference
outputs = session.run(None, {'input': input_data})

# Results: 4,010 samples/sec at batch=64
```

---

## ðŸ“Š Detailed Performance Analysis

### Optimization Impact Breakdown

| Technique | FLOP Reduction | Throughput Gain | Memory Savings | Sensitivity Impact |
|-----------|----------------|-----------------|----------------|-------------------|
| Interpolation Removal | 91.8% | 2.6x | Moderate | +0.77% (improved!) |
| Channels Last Format | - | Minor | Low | No change |
| In-Place ReLU | - | Minor | Low | No change |
| **FP16 Mixed Precision** | - | 1.3x | 50% | -0.26% (negligible) |
| **Combined** | **91.8%** | **31.5x** | **91%** | **+0.51%** |

### Batch Size Performance (ONNX Runtime)

| Batch Size | Latency (ms) | Throughput (sps) | Use Case |
|------------|--------------|------------------|----------|
| 1 | 3.74 | 268 | Real-time emergency diagnosis |
| 8 | 6.62 | 1,209 | Small clinic workflow |
| 16 | 9.73 | 1,645 | Medium hospital screening |
| 32 | 16.01 | 1,999 | High-volume clinic |
| **64** | **15.96** | **4,010** | **Bulk retrospective analysis** |

### Clinical Performance Validation

| Model | Accuracy | Precision | Sensitivity | Specificity | AUC |
|-------|----------|-----------|-------------|-------------|-----|
| Baseline | 95.8% | 94.2% | 98.46% | 93.1% | 97.3% |
| **Optimized** | **96.2%** | **94.8%** | **99.23%** | **93.4%** | **97.8%** |
| FP16 ONNX | 96.1% | 94.5% | 98.97% | 93.2% | 97.5% |

**Key Insight:** Optimization **improved** clinical accuracy - native resolution processing preserved diagnostic features better than interpolated inputs.

---

## ðŸŽ“ Key Learnings & Insights

### What Worked Exceptionally Well
1. **Data-driven optimization:** Understanding that the dataset is naturally 64Ã—64 led to the 91.8% FLOP reduction
2. **Clinical safety maintained:** All optimizations preserved >98% sensitivity requirement
3. **Batch size flexibility:** Dynamic batching enabled diverse deployment workflows without separate models
4. **Cross-platform strategy:** ONNX format provided deployment flexibility across GPU, CPU, and mobile

### Optimization Trade-offs
- **FLOP reduction vs latency:** 91.8% FLOP reduction didn't directly translate to 91.8% latency improvement due to memory bandwidth, Python overhead, and hardware limitations
- **FP16 precision:** Minimal sensitivity drop (-0.26%) justified 50% memory reduction and throughput gains
- **Batch size:** Higher batches improve throughput (4,010 sps) but increase latency (15.96 ms) - workflow-dependent optimization

### When to Stop Optimizing
- **Architecture optimization first:** Achieved 91.8% FLOP reduction before considering hardware acceleration
- **Hardware limitations:** Further optimization requires infrastructure changes (GPU with Tensor Cores, TensorRT) rather than model changes
- **Diminishing returns:** The 0.74 ms latency gap requires GPU deployment, not additional model compression
- **Production readiness:** 4/5 targets enables deployment with workflow adjustments (request batching for throughput-critical use cases)

---

## ðŸ”® Future Improvements

### Immediate Next Steps
- **Deploy on GPU with TensorRT:** Expected to meet 5/5 targets with kernel fusion and FP16 Tensor Cores
- **Implement request batching:** Aggregate emergency requests every 10ms to achieve <3ms effective latency
- **A/B testing framework:** Compare optimized model against baseline in hospital pilot deployment

### Advanced Optimizations
- **INT8 Quantization:** Further 50% model size reduction for mobile deployment (with careful sensitivity validation)
- **Knowledge Distillation:** Train smaller student model (ResNet-9) using optimized ResNet-18 as teacher
- **Neural Architecture Search:** Explore custom architectures optimized for 64Ã—64 medical imaging
- **Dynamic Batch Aggregation:** Implement Triton dynamic batching for automatic request aggregation

---

## ðŸ“š References & Resources

### Technical Documentation
- [PyTorch Performance Tuning Guide](https://pytorch.org/tutorials/recipes/recipes/tuning_guide.html)
- [ONNX Runtime Performance Optimization](https://onnxruntime.ai/docs/performance/)
- [NVIDIA TensorRT Documentation](https://docs.nvidia.com/deeplearning/tensorrt/)
- [Intel OpenVINO Toolkit](https://docs.openvino.ai/)

### Medical AI Resources
- [PneumoniaMNIST Dataset](https://medmnist.com/)
- [ResNet Architecture Paper](https://arxiv.org/abs/1512.03385)
- [Medical AI Deployment Best Practices](https://arxiv.org/abs/2109.09824)

### Related Projects
- [TorchServe Model Deployment](https://pytorch.org/serve/)
- [Triton Inference Server](https://github.com/triton-inference-server/server)
- [ONNX Model Zoo](https://github.com/onnx/models)

---

## ðŸ“§ Contact & Portfolio

**Author:** Saurabh Bhardwaj

**LinkedIn:** [https://www.linkedin.com/in/saurabhbhardwajofficial/](https://www.linkedin.com/in/saurabhbhardwajofficial/)

**Portfolio:** [https://bhardwaj-saurabh.github.io/](https://bhardwaj-saurabh.github.io/)

## ðŸ“„ License

This project is licensed under the MIT License - see the [LICENSE.md](LICENSE.md) file for details.

---

## ðŸ™ Acknowledgments

- **Udacity** for project framework and medical AI deployment curriculum
- **MedMNIST** team for curated medical imaging dataset
- **PyTorch** and **ONNX Runtime** communities for optimization tools
- **Hospital partners** for clinical validation requirements and deployment insights

---

**â­ If this project helped you understand medical AI optimization, please star the repository!**

**ðŸ’¼ Open to ML Engineering opportunities in Healthcare AI, Model Optimization, and Production ML Systems.**
